# Reddit URL Scraper - Manual de Usuario

Sistema para extraer y organizar URLs compartidas en subreddits de Reddit.

## ğŸ¯ Â¿QuÃ© hace?

Recopila automÃ¡ticamente **todas las URLs** (enlaces web) de posts en los subreddits que configures:
- Proyectos, startups, herramientas
- Apps, sitios web, demos
- Todo se guarda en una base de datos
- Dashboard web para ver y buscar fÃ¡cilmente

## âœ¨ CaracterÃ­sticas

- ğŸ“Š **Dashboard Visual** - Interfaz web fÃ¡cil de usar
- ğŸ” **BÃºsqueda** - Encuentra URLs por palabra clave
- ğŸ“¥ **Exportar a Excel** - Descarga datos en CSV
- ğŸ”„ **ActualizaciÃ³n Diaria** - Solo obtiene posts nuevos
- ğŸ“š **Historial Completo** - Puede obtener posts de hasta 6 meses
- ğŸš« **Sin duplicados** - No guarda la misma URL dos veces
- ğŸ”“ **Sin cuenta Reddit** - No requiere login

## ğŸ“‹ Requisitos Previos (Windows)

1. **Python 3.8 o superior**
   - Descargar de: https://www.python.org/downloads/
   - âš ï¸ **IMPORTANTE:** Marcar "Add Python to PATH" durante instalaciÃ³n

2. **Git** (opcional, para actualizaciones)
   - Descargar de: https://git-scm.com/download/win

---

## ğŸš€ InstalaciÃ³n en Windows

### Paso 1: Descargar el Proyecto

**OpciÃ³n A - Con Git:**
```powershell
git clone https://github.com/saun1790/Reddit-URL-Scraping.git
cd Reddit-URL-Scraping
```

**OpciÃ³n B - Sin Git:**
1. Ve a: https://github.com/saun1790/Reddit-URL-Scraping
2. Clic en botÃ³n verde "Code" â†’ "Download ZIP"
3. Descomprime el archivo
4. Abre PowerShell en esa carpeta (Shift + Click derecho â†’ "Abrir PowerShell aquÃ­")

### Paso 2: Instalar Dependencias

```powershell
# Si te da error de permisos, ejecuta esto primero:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Crear entorno virtual
python -m venv venv

# Instalar librerÃ­as
.\venv\Scripts\pip install -r requirements.txt
```

âœ… **Â¡InstalaciÃ³n completa!**

---

## ğŸ–¥ï¸ Usar el Dashboard

### Iniciar el Sistema

```powershell
.\venv\Scripts\python web_viewer.py
```

VerÃ¡s algo como:
```
 * Running on http://127.0.0.1:3010
```

Abre tu navegador en: **http://localhost:3010**

### Configurar Subreddits

1. Clic en **âš™ï¸ Settings** (esquina superior derecha)
2. Escribe el nombre del subreddit **sin** "r/" (ejemplo: `SideProject`)
3. Presiona Enter o clic en "+"
4. Para eliminar: clic en âŒ al lado del nombre

**Subreddits recomendados:**
- `SideProject` - Proyectos personales
- `startups` - Startups y emprendimiento  
- `entrepreneur` - Negocios
- `InternetIsBeautiful` - Sitios web interesantes

### Obtener URLs

1. Clic en **âš¡ Fetch URLs**
2. Selecciona modo:
   - **Daily** (rÃ¡pido, 1-2 min) - Solo posts nuevos
   - **Backfill** (lento, 5-10 min) - Posts histÃ³ricos
3. Clic en **Start**
4. Espera a que termine

### Buscar y Filtrar

- **BÃºsqueda:** Escribe palabra clave (ej: "AI", "SaaS")
- **Filtro:** Desplegable para ver solo un subreddit
- **Exportar:** BotÃ³n "ğŸ“¥ Export CSV" descarga todo en Excel

---

## ğŸ’» Uso Desde LÃ­nea de Comandos

### ActualizaciÃ³n Diaria (Recomendado)

```powershell
.\venv\Scripts\python reddit_scraper_noauth.py --daily --subreddits SideProject startups
```

### Obtener HistÃ³rico (Primera Vez)

```powershell
# Ãšltimos 30 dÃ­as
.\venv\Scripts\python reddit_scraper_noauth.py --backfill 30 --subreddits SideProject

# Ãšltimos 6 meses
.\venv\Scripts\python reddit_scraper_noauth.py --backfill 180 --subreddits SideProject startups
```

### Exportar a CSV

```powershell
.\venv\Scripts\python reddit_scraper_noauth.py --export urls.csv
```

### Ver EstadÃ­sticas

```powershell
.\venv\Scripts\python reddit_scraper_noauth.py --stats
```

---

## ğŸ”„ Actualizar el Sistema

Si hay una versiÃ³n nueva disponible:

```powershell
# Con Git
git pull

# Reinstalar dependencias (si hubo cambios)
.\venv\Scripts\pip install -r requirements.txt --upgrade
```

---

## ğŸ“Š Estructura de Datos

Los datos se guardan en `reddit_urls.db` (base de datos SQLite)

| Campo | DescripciÃ³n |
|-------|-------------|
| `url` | Enlace web encontrado en el post |
| `post_date` | Fecha del post (UTC) |
| `subreddit` | De quÃ© subreddit viene |
| `post_id` | ID del post en Reddit |

---

## ğŸ†˜ SoluciÃ³n de Problemas

### "Puerto 3010 ya estÃ¡ en uso"

Significa que ya tienes el dashboard abierto. Cierra la ventana anterior o:

```powershell
# Ver quÃ© estÃ¡ usando el puerto
netstat -ano | findstr :3010

# Matar el proceso (reemplaza PID con el nÃºmero que aparece)
taskkill /PID <numero> /F
```

### "ModuleNotFoundError: No module named 'flask'"

Reinstala las dependencias:

```powershell
.\venv\Scripts\pip install -r requirements.txt
```

### "Error de permisos al activar venv"

Ejecuta primero:

```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### Scraping muy lento

- Usa **Daily** en lugar de **Backfill**
- Reduce el nÃºmero de dÃ­as en Backfill
- Verifica tu conexiÃ³n a internet

### No encuentra URLs nuevas

Posibles causas:
- No hay posts nuevos en ese subreddit
- Ya tienes todos los posts recientes
- El subreddit estÃ¡ inactivo

**SoluciÃ³n:** Prueba otro subreddit mÃ¡s activo

---

## ğŸ“ Archivos del Proyecto

```
Reddit-URL-Scraping/
â”œâ”€â”€ web_viewer.py             # Dashboard web
â”œâ”€â”€ reddit_scraper_noauth.py  # Scraper (lÃ­nea de comandos)
â”œâ”€â”€ database.py               # Manejo de base de datos
â”œâ”€â”€ requirements.txt          # LibrerÃ­as necesarias
â”œâ”€â”€ USER_GUIDE.md            # GuÃ­a completa de usuario (NO TÃ‰CNICA)
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html           # Interfaz del dashboard
â””â”€â”€ reddit_urls.db           # Base de datos (se crea automÃ¡ticamente)
```

---

## â“ Preguntas Frecuentes

**Â¿Necesito una cuenta de Reddit?**  
No, el sistema funciona sin autenticaciÃ³n.

**Â¿CuÃ¡ntos subreddits puedo agregar?**  
Todos los que quieras, pero recomendamos 3-5 para empezar.

**Â¿Los datos se guardan permanentemente?**  
SÃ­, todo se guarda en `reddit_urls.db`. No se pierde al cerrar.

**Â¿Puedo usar esto en otra computadora?**  
SÃ­, copia la carpeta completa (incluye el archivo `.db`).

**Â¿Cada cuÃ¡nto debo ejecutar el scraper?**  
Depende de tus necesidades:
- Diario si quieres estar al dÃ­a
- Semanal si solo revisas periÃ³dicamente
- Cuando lo necesites

---

## ğŸ“š DocumentaciÃ³n Adicional

- **USER_GUIDE.md** - GuÃ­a completa para usuarios (recomendado)
- **SUPER_SCRAPING_AGENT_PROMPT.md** - DocumentaciÃ³n tÃ©cnica avanzada

---

## ğŸ“ Soporte

Si tienes problemas:
1. Revisa esta guÃ­a primero
2. Consulta **USER_GUIDE.md** para mÃ¡s detalles
3. Verifica que tienes la Ãºltima versiÃ³n (`git pull`)

---

**Ãšltima actualizaciÃ³n:** Enero 2026  
**VersiÃ³n:** 1.0  
**Licencia:** MIT
