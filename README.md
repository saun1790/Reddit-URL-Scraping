# ğŸ”— Reddit URL Scraper

Sistema profesional para extraer URLs externas de posts de Reddit. Incluye dashboard web interactivo y capacidad de backfill hasta 6 meses de datos histÃ³ricos.

---

## ğŸ“‹ Requisitos

- **Python 3.8+**
- **macOS / Linux / Windows**
- ConexiÃ³n a Internet

---

## âš¡ InstalaciÃ³n RÃ¡pida

### 1. Instalar Dependencias

```bash
cd reddit_scraper
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. ConfiguraciÃ³n Inicial (Primera vez)

El scraper NO requiere API keys de Reddit. Usa el API pÃºblico sin autenticaciÃ³n.

---

## ğŸš€ Uso

### OpciÃ³n 1: Dashboard Web (Recomendado)

**Iniciar el servidor:**

```bash
cd reddit_scraper
source venv/bin/activate
python web_viewer.py
```

**Abrir en navegador:**

```
http://localhost:3010
```

#### Funcionalidades del Dashboard:

- âœ… **Ver todas las URLs** con bÃºsqueda y filtros
- âœ… **Ordenar columnas** haciendo clic en las cabeceras
- âœ… **Redimensionar columnas** arrastrando los bordes
- âœ… **Fetch URLs** con progreso en tiempo real
- âœ… **Exportar a CSV** con un clic
- âœ… **Configurar subreddits** desde el menÃº Settings

#### CÃ³mo usar el Fetch:

1. Clic en **"âš¡ Fetch URLs"**
2. Seleccionar modo:
   - **Daily**: Solo posts nuevos desde la Ãºltima ejecuciÃ³n
   - **Backfill**: HistÃ³rico (hasta 180 dÃ­as / ~6 meses)
3. Configurar dÃ­as si es Backfill
4. Ver progreso en tiempo real
5. Â¡Listo! Los datos se actualizan automÃ¡ticamente

---

### OpciÃ³n 2: LÃ­nea de Comandos

#### Primera vez - Obtener datos histÃ³ricos (6 meses):

```bash
python reddit_scraper_noauth.py --backfill 180 --subreddits SideProject
```

#### Uso diario - Solo datos nuevos:

```bash
python reddit_scraper_noauth.py --daily --subreddits SideProject
```

#### MÃºltiples subreddits:

```bash
python reddit_scraper_noauth.py --backfill 90 --subreddits SideProject startups entrepreneur
```

#### Exportar a CSV:

```bash
python reddit_scraper_noauth.py --export mi_archivo.csv
```

#### Ver estadÃ­sticas:

```bash
python reddit_scraper_noauth.py --stats
```

---

## ğŸ“Š Datos ExtraÃ­dos

El sistema guarda:

| Campo | DescripciÃ³n | Ejemplo |
|-------|-------------|---------|
| **url** | URL externa encontrada | https://example.com |
| **post_date** | Fecha del post | 2026-01-29 10:30:15 |
| **subreddit** | Subreddit de origen | SideProject |
| **post_id** | ID Ãºnico del post | 1qq7qfq |

**Base de datos:** `reddit_urls.db` (SQLite)

**ExportaciÃ³n CSV:** Formato estÃ¡ndar compatible con Excel/Google Sheets

---

## âš™ï¸ Configurar Subreddits

### Desde el Dashboard:

1. Clic en **"âš™ï¸ Settings"**
2. Agregar o quitar subreddits
3. Se guardan automÃ¡ticamente

### Desde lÃ­nea de comandos:

Simplemente lista los subreddits al ejecutar:

```bash
python reddit_scraper_noauth.py --daily --subreddits SideProject startups entrepreneur
```

---

## ğŸ”„ AutomatizaciÃ³n (Opcional)

### Ejecutar automÃ¡ticamente cada dÃ­a:

**macOS/Linux - Cron:**

```bash
crontab -e
```

Agregar esta lÃ­nea (reemplaza la ruta):

```bash
0 9 * * * cd /ruta/a/reddit_scraper && ./venv/bin/python reddit_scraper_noauth.py --daily --subreddits SideProject
```

Esto ejecutarÃ¡ el scraper todos los dÃ­as a las 9:00 AM.

**Windows - Task Scheduler:**

1. Abrir "Programador de tareas"
2. Crear tarea bÃ¡sica
3. AcciÃ³n: Ejecutar programa
4. Programa: `C:\ruta\a\venv\Scripts\python.exe`
5. Argumentos: `reddit_scraper_noauth.py --daily --subreddits SideProject`
6. Carpeta de inicio: `C:\ruta\a\reddit_scraper`

---

## ğŸ“ˆ Capacidades

| CaracterÃ­stica | Detalle |
|----------------|---------|
| **Datos histÃ³ricos** | Hasta 6 meses (~180 dÃ­as) |
| **Subreddits simultÃ¡neos** | Ilimitados |
| **Sin duplicados** | Constraint Ãºnico en base de datos |
| **Rate limiting** | Respeta lÃ­mites de Reddit automÃ¡ticamente |
| **URLs procesadas** | Filtra links internos de Reddit |
| **ActualizaciÃ³n diaria** | Solo trae posts nuevos |

---

## ğŸ¯ Casos de Uso

- Monitorear lanzamientos de productos en r/SideProject
- Seguir trends en r/startups, r/entrepreneur
- AnÃ¡lisis de contenido externo compartido
- Lead generation de nuevos proyectos
- Research de competencia

---

## ğŸ› ï¸ Comandos Ãštiles

```bash
# Ver cuÃ¡ntas URLs tienes
python reddit_scraper_noauth.py --stats

# Exportar todo a CSV
python reddit_scraper_noauth.py --export reddit_urls_$(date +%Y%m%d).csv

# Backfill 3 meses de varios subreddits
python reddit_scraper_noauth.py --backfill 90 --subreddits SideProject startups entrepreneur

# Iniciar dashboard
python web_viewer.py
```

---

## ğŸ“‚ Estructura de Archivos

```
reddit_scraper/
â”œâ”€â”€ reddit_scraper_noauth.py  # Script principal
â”œâ”€â”€ web_viewer.py              # Dashboard web
â”œâ”€â”€ database.py                # GestiÃ³n de SQLite
â”œâ”€â”€ reddit_urls.db            # Base de datos
â”œâ”€â”€ requirements.txt           # Dependencias Python
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            # Frontend del dashboard
â””â”€â”€ README.md                 # Este archivo
```

---

## â“ Problemas Comunes

### El dashboard no carga:

```bash
# Verificar que el puerto 3010 estÃ© libre
lsof -i :3010

# Reiniciar el servidor
pkill -f web_viewer.py
python web_viewer.py
```

### No encuentra posts antiguos:

Reddit API tiene lÃ­mites. Con mÃºltiples endpoints podemos obtener ~6 meses para subreddits activos.

### Error de conexiÃ³n:

Verificar conexiÃ³n a internet y que Reddit no estÃ© caÃ­do.

---

## ğŸ“ Soporte

Para dudas o problemas:

1. Verificar que el entorno virtual estÃ© activado: `source venv/bin/activate`
2. Ver logs en la terminal donde corre el servidor
3. Revisar `reddit_urls.db` con `sqlite3 reddit_urls.db`

---

## ğŸ‰ Â¡Listo!

Ya tienes todo configurado. Abre **http://localhost:3010** y empieza a extraer URLs de Reddit.

**RecomendaciÃ³n:** Ejecuta un backfill de 90-180 dÃ­as la primera vez, luego usa modo `--daily` para mantener actualizado.
