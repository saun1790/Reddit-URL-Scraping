# Reddit URL Scraper

Extract external URLs from Reddit subreddit posts. Includes web dashboard and historical backfill up to 6 months.

![Dashboard Preview](https://img.shields.io/badge/Dashboard-Live-green) ![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue)

## Features

- ðŸ”„ **Backfill mode**: Extract posts from last N days (up to 180 days)
- ðŸ“… **Daily mode**: Fetch only new posts since last run
- ðŸš« **No duplicates**: SQLite database with unique constraints
- ðŸ“Š **Multiple subreddits**: Track unlimited subreddits
- ðŸ“¥ **CSV export**: One click download
- ðŸ–¥ï¸ **Web dashboard**: Interactive UI with search, filters, and pagination
- ðŸ”“ **No API keys required**: Uses Reddit's public JSON endpoints

## Technologies

| Component | Technology |
|-----------|------------|
| Backend | Python 3.8+ |
| Web Framework | Flask |
| Database | SQLite |
| Reddit Data | Public JSON API (no auth) |
| Frontend | HTML5 / CSS3 / Vanilla JS |

## Prerequisites

### Linux (Ubuntu/Debian)
```bash
sudo apt update
sudo apt install python3 python3-venv python3-pip git -y
```

### Linux (CentOS/RHEL/Fedora)
```bash
sudo dnf install python3 python3-pip git -y
```

### macOS
```bash
brew install python git
```

### Windows
1. Download Python from https://www.python.org/downloads/
2. **IMPORTANT:** Check "Add Python to PATH" during installation
3. Download Git from https://git-scm.com/download/win

## Installation

### Linux / macOS
```bash
git clone https://github.com/saun1790/Reddit-URL-Scraping.git
cd Reddit-URL-Scraping
python3 -m venv venv
./venv/bin/pip install -r requirements.txt
```

### Windows (PowerShell)
```powershell
git clone https://github.com/saun1790/Reddit-URL-Scraping.git
cd Reddit-URL-Scraping
python -m venv venv
.\venv\Scripts\pip install -r requirements.txt
```

## Quick Start

### Start the Web Dashboard

**Linux / macOS:**
```bash
./venv/bin/python web_viewer.py
```

**Windows:**
```powershell
.\venv\Scripts\python web_viewer.py
```

Open your browser: **http://localhost:3010**

### Dashboard Features

1. **âš™ï¸ Settings** - Configure which subreddits to track (comma-separated, without r/)
2. **âš¡ Fetch URLs** - Run scraper in Daily or Backfill mode
3. **ðŸ” Search** - Filter URLs by keyword
4. **ðŸ“¥ Export CSV** - Download all data

## Command Line Usage

### Backfill (Historical Data)
```bash
# Last 30 days
./venv/bin/python reddit_scraper_noauth.py --backfill 30 --subreddits SideProject

# Last 6 months, multiple subreddits
./venv/bin/python reddit_scraper_noauth.py --backfill 180 --subreddits SideProject startups entrepreneur
```

### Daily Update
```bash
./venv/bin/python reddit_scraper_noauth.py --daily --subreddits SideProject
```

### Export to CSV
```bash
./venv/bin/python reddit_scraper_noauth.py --export urls.csv
```

### View Statistics
```bash
./venv/bin/python reddit_scraper_noauth.py --stats
```

## Running in Background

### Linux / macOS
```bash
# Start
nohup ./venv/bin/python web_viewer.py > web_viewer.log 2>&1 &
echo $! > web_viewer.pid

# Stop
kill $(cat web_viewer.pid)

# View logs
tail -f web_viewer.log
```

### Windows (PowerShell)
```powershell
# Start
Start-Process -WindowStyle Hidden -FilePath ".\venv\Scripts\python.exe" -ArgumentList "web_viewer.py"

# Stop
Get-Process python | Stop-Process
```

## Automation (Cron)

Run daily at 9 AM:
```bash
crontab -e
```

Add:
```bash
0 9 * * * cd /path/to/Reddit-URL-Scraping && ./venv/bin/python reddit_scraper_noauth.py --daily --subreddits SideProject >> cron.log 2>&1
```

## Data Structure

| Field | Description |
|-------|-------------|
| `url` | External URL found in post |
| `post_date` | Post timestamp (UTC) |
| `subreddit` | Source subreddit |
| `post_id` | Reddit post ID |

Database file: `reddit_urls.db` (SQLite, created on first run)

## Project Structure

```
Reddit-URL-Scraping/
â”œâ”€â”€ web_viewer.py             # Web dashboard server
â”œâ”€â”€ reddit_scraper_noauth.py  # Main scraper (CLI)
â”œâ”€â”€ database.py               # SQLite database handler
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            # Dashboard UI
â””â”€â”€ reddit_urls.db            # Database (auto-created)
```

## Troubleshooting

**Port 3010 already in use:**
```bash
# Find and kill process
lsof -i :3010
kill -9 <PID>
```

**Permission denied:**
```bash
chmod +x ./venv/bin/python
```

**Module not found:**
```bash
./venv/bin/pip install -r requirements.txt
```

## License

MIT

## Linux Service (systemd)

Create a systemd service to run the web dashboard automatically on boot.

### 1. Create Service File

```bash
sudo nano /etc/systemd/system/reddit-scraper.service
```

Paste the following (adjust paths and user):

```ini
[Unit]
Description=Reddit URL Scraper Web Dashboard
After=network.target

[Service]
Type=simple
User=YOUR_USERNAME
WorkingDirectory=/home/YOUR_USERNAME/Reddit-URL-Scraping
ExecStart=/home/YOUR_USERNAME/Reddit-URL-Scraping/venv/bin/python web_viewer.py
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

> **Note:** Replace `YOUR_USERNAME` with your actual username and adjust paths if needed.

### 2. Enable and Start

```bash
# Reload systemd
sudo systemctl daemon-reload

# Enable on boot
sudo systemctl enable reddit-scraper

# Start service
sudo systemctl start reddit-scraper
```

### 3. Service Commands

| Command | Description |
|---------|-------------|
| `sudo systemctl start reddit-scraper` | Start the service |
| `sudo systemctl stop reddit-scraper` | Stop the service |
| `sudo systemctl restart reddit-scraper` | Restart the service |
| `sudo systemctl status reddit-scraper` | Check status |
| `sudo journalctl -u reddit-scraper -f` | View live logs |
| `sudo journalctl -u reddit-scraper --since today` | Today's logs |

### 4. Optional: Daily Scraper Service (Timer)

Create a timer to run the scraper daily:

```bash
sudo nano /etc/systemd/system/reddit-scraper-daily.service
```

```ini
[Unit]
Description=Reddit URL Scraper Daily Fetch
After=network.target

[Service]
Type=oneshot
User=YOUR_USERNAME
WorkingDirectory=/home/YOUR_USERNAME/Reddit-URL-Scraping
ExecStart=/home/YOUR_USERNAME/Reddit-URL-Scraping/venv/bin/python reddit_scraper_noauth.py --daily --subreddits SideProject
```

Create the timer:

```bash
sudo nano /etc/systemd/system/reddit-scraper-daily.timer
```

```ini
[Unit]
Description=Run Reddit Scraper Daily at 9 AM

[Timer]
OnCalendar=*-*-* 09:00:00
Persistent=true

[Install]
WantedBy=timers.target
```

Enable the timer:

```bash
sudo systemctl daemon-reload
sudo systemctl enable reddit-scraper-daily.timer
sudo systemctl start reddit-scraper-daily.timer

# Check timer status
systemctl list-timers | grep reddit
```

### Quick Setup Script

Save this as `install-service.sh` and run with `sudo`:

```bash
#!/bin/bash
set -e

# Configuration
USER=$(whoami)
APP_DIR=$(pwd)

echo "ðŸ“¦ Installing Reddit Scraper service..."
echo "   User: $USER"
echo "   Directory: $APP_DIR"

# Create web dashboard service
cat > /etc/systemd/system/reddit-scraper.service << SERVICEEOF
[Unit]
Description=Reddit URL Scraper Web Dashboard
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$APP_DIR
ExecStart=$APP_DIR/venv/bin/python web_viewer.py
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
SERVICEEOF

# Reload and enable
systemctl daemon-reload
systemctl enable reddit-scraper
systemctl start reddit-scraper

echo "âœ… Service installed!"
echo ""
echo "Commands:"
echo "  sudo systemctl status reddit-scraper"
echo "  sudo systemctl restart reddit-scraper"
echo "  sudo journalctl -u reddit-scraper -f"
```

Run:
```bash
chmod +x install-service.sh
sudo ./install-service.sh
```
